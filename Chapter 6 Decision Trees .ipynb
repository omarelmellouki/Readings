{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Vizualizing a Decision Tree\n",
    "\n",
    "To understand Decision Trees we start by building one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the decision Tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:] # We get the petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting a graph\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "f = open(\"/home/omar/Bureau/apprentissage/GÃ©ron/Chapter_6_ressources/iris_tree.dot\", 'w')\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file = f,\n",
    "    feature_names = iris.feature_names[2:],\n",
    "    class_names = iris.target_names,\n",
    "    rounded = True,\n",
    "    filled = True\n",
    ")\n",
    "\n",
    "# Command to convert to a png : dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "# Command to open it : eog <img>.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "![Decision Tree](Chapter_6_ressources/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start at the root node. If the petal length is <= 2.45 we go left. The left node happens to be a leaf node, meaning without any children, so it simply predicts the class it has, the $setosa$. On the contrary, if it has a petal length greater than 2.45 it goes right, then checks the condition about the width and the predicts.\n",
    "\n",
    "A nodes samples attribute is the number of nodes it applies to. The Values attribute gives how many instances of each class the node applies to. The Gini coefficient gives the purity/impurity of the node. A node is pure if all the instances it applies to are in the same class. The gini coefficient is 1 - the ratios of the instances in class k over the samples of node i. \n",
    "\n",
    "The Class Probabilities are given by this ratio. It is the same anywhere in the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART training algorithm\n",
    "\n",
    "The Cart training algorithm starts by splitting the data set according to a single feature k. It also assigns a threshold to this feature. The class and threshold is searched in order to produce the purest subsets, and the cost function is minimized iteratively over the nodes. \n",
    "\n",
    "The CART algorithm is a greedy algorithm: it searches for an optimum split at the top level then repeats the process at each level. \n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "## Gini Impurity or Entropy\n",
    "\n",
    "The Gini Impurity is used by default but we can choose to use entropy. Both produce similar results, while Gini is faster to compute and tends to isolate the most frequent class in its own branch while the entropy produces more balanced trees. \n",
    "\n",
    "## Regularization Hyperparameters \n",
    "\n",
    "Decision Trees make very few assumptions about the training unlike for example the linear regression where the data is assumed to be linear.\n",
    "\n",
    "In the decision tree, a non parametric model, the number of parameters is not predetermined so its easy to overfit the data. To avoid doing so, it is possible to restrict the models parameters, by for instance reducing maximum depth of the model.\n",
    "\n",
    "# Regression \n",
    "\n",
    "The regression tree is very similar to the classification tree. The value predicted is not the most frequent class, it is rather the average value of the instances in the node. \n",
    "\n",
    "The training is the same, except it aims to minimize the mse. It is important to set the minimum number of samples per leafs to avoid overfitting, which will happen without regularization. \n",
    "\n",
    "# Instability\n",
    "\n",
    "Decision Trees love orthogonal decision boundaries which makes them sensitive to training set rotation. They are also very sensitive to data variation, leading to a big variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'max_leaf_nodes': 15}\n",
      "Accuracy : 0.8511875908870576\n"
     ]
    }
   ],
   "source": [
    "# Training and fine tuning a decision tree on the moons data set\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "moons = make_moons(n_samples = 10000, noise = 0.4)\n",
    "train_X, test_X, train_y, test_y = train_test_split(moons[0],moons[1], test_size = 0.2)\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "param_search = [{'max_leaf_nodes': [3,5,7,9,11,13,15]},]\n",
    "\n",
    "grid_search = GridSearchCV(tree_clf, param_search, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "tree_clf = grid_search.best_estimator_\n",
    "params = grid_search.best_params_\n",
    "print(\"Best Parameters :\", params)\n",
    "\n",
    "prediction = tree_clf.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Accuracy :\",f1_score(prediction, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Growing a forest \n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(train_X) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(train_X):\n",
    "    X_mini_train = train_X[mini_train_index]\n",
    "    y_mini_train = train_y[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.7952453531994995\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "\n",
    "from sklearn.base import clone\n",
    "tree = clone(grid_search.best_estimator_)\n",
    "for sets in mini_sets:\n",
    "    tree.fit(sets[0],sets[1])\n",
    "    pred = tree.predict(test_X)\n",
    "    precisions.append(f1_score(pred, test_y))\n",
    "\n",
    "import numpy as np\n",
    "print(\"Precision :\", np.mean(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8476953907815632\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "for sets in mini_sets:\n",
    "    tree = clone(grid_search.best_estimator_)\n",
    "    tree.fit(sets[0],sets[1])\n",
    "    pred = tree.predict(test_X)\n",
    "    predictions.append(pred)\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "common = mode(predictions, axis=0)\n",
    "common = common[0].tolist()\n",
    "print(\"Accuracy :\", f1_score(common[0], test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
