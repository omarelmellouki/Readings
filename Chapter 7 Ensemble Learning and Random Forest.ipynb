{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the idea of crowd wisdom, Emsemble Methods agreagate the predictions of a group of predictors, and often get a predictions that is better than each individual predictor. The winning solutions in Machine Learning competitions often involve several Ensemble Methods (netflixprize.com). \n",
    "\n",
    "# Voting classifiers\n",
    "\n",
    "Suppose we have many models, a simple way to aggregate the results is to pick the majority class : this is called a hard voting classifier. \n",
    "\n",
    "This aggregated model often performs way better than the separate models. In fact even if most of the classifiers are weak classifiers, the aggregated classifier is a strong classifier, provided there are sufficient numbers of weak classifiers and are sufficiently diverse. \n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to do so is to train them using different algorithms. This increases the chances that they'll make different types of errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Exemple of a voting classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting = 'soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting \n",
    "\n",
    "Instead of training different models on the same data set we can train the same model on different samples of the same data set. \n",
    "\n",
    "If the sample are built with replement the method is called bagging. Otherwise it is called pasting. \n",
    "\n",
    "## Bagging and Pasting in Scikit-Learn\n",
    "\n",
    "The following code trains 500 Decision Trees classfiers each on 100 training instances randomly sampled from the training set with replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.451801538467407\n",
      "Bagged:  0.912\n",
      "OoB Score:  0.9253333333333333\n",
      "Tree:  0.856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "bag_clf = BaggingClassifier(\n",
    "    \n",
    "    DecisionTreeClassifier(), n_estimators = 500, \n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1,oob_score= True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "tac = time.time() #67 seconds for 50k trees without parallel processing, 29.4 seconds with parallel processing \n",
    "print(tac-tic)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Bagged: \", accuracy_score(y_test,y_pred))\n",
    "print(\"OoB Score: \", bag_clf.oob_score_)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Tree: \", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports ampling the features as well. This is very useful if you have a lot of features like when dealing with images for instance. Sampling both inputs and features is called Random Patches method. Keeping all instances and sampling only features is called Random Subspaces. \n",
    "\n",
    "# Random Forests\n",
    "\n",
    "The random forest algorithm introduces more randomness when building the tree : instead of searching for the very best feature it searches rather for the best feature among a random subset of features. \n",
    "\n",
    "Extreemly Randomized Trees are also a way to build a random forest by selecting a random threshold rather than searching for the very best threshold. \n",
    "\n",
    "Random Forest Classifiers also compute the importance of each feature by looking at how much the tree nodes that use tha feature reduce impurity. It is a very handy way to get a quick understanding of what features actually matter and in particular if we need to perform feature selection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
