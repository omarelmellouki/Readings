{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "\n",
    "The linear SVM Classifier will fit the widest street between the classes. It is called large margin classifier. This will set a \"stree\" between that classes that is \"supported\" (hence the name) by the two instances (vectors) located on the edge of the classes. \n",
    "\n",
    "This classifier is very sensitive to outliers and scaling values. \n",
    "To correct this problem, we use soft margin classifier.\n",
    "\n",
    "## Soft Margin Classifier\n",
    "\n",
    "If we strictly impose that all instances must be off the street and on the right side we call that hard margin classifier. This only works if the data is linearily separable and it is very sensitive to outliers. \n",
    "\n",
    "In soft margin we try to find a good balance between keeping the streets as large as possible and limiting the margin violations. This is done by the $C$ parameter. If the model is overfitting we can try to regularize by reducing C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:,(2,3)] # We get width and length\n",
    "y = (iris[\"target\"]==2).astype(np.float64)\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X,y)\n",
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non linear SVM Classifier \n",
    "\n",
    "One way to deal with non linear datasets is to add variables such as polynomial variables that will transform the data sets and make the linearily seperable. To implement this idea one could add a polynomial transformer to the data preprocessing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('polynomial', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge', max_iter=2000))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X,y = make_moons(n_samples = 100, noise = 0.15)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    \n",
    "    (\"polynomial\", PolynomialFeatures(degree = 3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss='hinge', max_iter = 2000))\n",
    "    \n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "The polynomial kernel allows the SVM to get the same result as adding many polynomial features without actually adding it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Features \n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a similarity function. By doing so you can add dimensions to the problem, and when turning each instance into its distance to a landmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing SVM for Regression\n",
    "\n",
    "Instead of trying to fit the larget street while keeping as much points out of it, in regression you try to fit as much points as possible in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree = 2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the hood\n",
    "\n",
    "We use the convention with the weigths and biases. \n",
    "\n",
    "## Decision Function and Predictions\n",
    "\n",
    "The linear SVMCLassifier computes the decision function $w^T.x + b$. If the result is positifve the predicted class is the positive class and otherwise it is the negative class. \n",
    "\n",
    "Training a linear SVM Classifier means finding the values of w to make this margin as wide a spossible while avoiding margin violations or limiting them. \n",
    "\n",
    "## Training objective\n",
    "\n",
    "We need to minimize ||w|| in order to get a large margin. If we also want to avoid margin violations we need to add a constraint with regards to the sign of the decision funcion (not understood). \n",
    "\n",
    "## Quadratic Programming \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
